# LitChatGLM2
Fine-tuning ğŸ¤–[ChatGLM2](https://github.com/THUDM/ChatGLM2-6B) with ğŸ¤—[HF-LoRA](https://github.com/huggingface/peft) and âš¡[Lightning](https://github.com/Lightning-AI/lightning)

## Data
```json lines
{"instruction": "å°†ä»¥ä¸‹å¥å­ä»ä¸€ç§æ—¶æ€è½¬åŒ–æˆå¦ä¸€ç§æ—¶æ€", "input": "ä»–æ­£åœ¨å¾€å•†åº—èµ°", "output": "ä»–æ›¾ç»å¾€å•†åº—èµ°è¿‡"}
{"instruction": "é’ˆå¯¹äº§å“å‘å¸ƒæå‡ºäº”ç§è¥é”€ç­–ç•¥ã€‚", "input": "", "output": "1. ç¤¾äº¤åª’ä½“æ´»åŠ¨ã€‚\n2. ç”µå­é‚®ä»¶è¥é”€ã€‚\n3. åœ¨çº¿å’Œç¦»çº¿å¹¿å‘Šã€‚\n4. æ¨èå’Œè¯„è®ºã€‚\n5. åˆä½œåäººæ¨é”€ã€‚"}
...
```

## Train
set `model.bits` to 4 for QLoRA
```shell
CUDA_VISIBLE_DEVICES=0 python run_chatglm2.py fit \
    --model.pretrained_model_name_or_path THUDM/chatglm2-6b \
    --model.bits 4 \
    --model.learning_rate 2e-5 \
    --data.train_batch_size 16 \
    --data.eval_batch_size 16 \
    --data.max_source_length  256 \
    --data.max_target_length  256 \
    --data.train_data_path PATH_TO_TRAIN_DATA \
    --data.val_data_path PATH_TO_VAL_DATA \
    --trainer.max_epoch 10 \
    --trainer.val_check_interval 1000 \
    --trainer.accumulate_grad_batches 1 \
    --trainer.precision bf16-mixed
  
```

## Infer
```shell
CUDA_VISIBLE_DEVICES=1 python generate.py \
    --llm_model_file PATH_TO_LLM \
    --peft_model_file PATH_TO_LORA \
    --inp_file PATH_TO_INPUT_FILE \
    --out_file PATH_TO_OUTPUT_FILE
```
set `ref_file` and `k_shot` for simple K-Shot in-context learning